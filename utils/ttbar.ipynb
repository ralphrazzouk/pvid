{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graph Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = \"\\033[95m\"\n",
    "    CYAN = \"\\033[96m\"\n",
    "    DARKCYAN = \"\\033[36m\"\n",
    "    BLUE = \"\\033[94m\"\n",
    "    GREEN = \"\\033[92m\"\n",
    "    YELLOW = \"\\033[93m\"\n",
    "    RED = \"\\033[91m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNDERLINE = \"\\033[4m\"\n",
    "    END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = False\n",
    "read = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample == True:\n",
    "    file = uproot.open(\"./datasets/ttbar_sample.root\")\n",
    "    tree = file[\"events\"]\n",
    "    branches = tree.arrays()\n",
    "else:\n",
    "    file = uproot.open(\"./datasets/ttbar.root\")\n",
    "    tree = file[\"Events\"]\n",
    "    branches = tree.arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(color.BOLD + \"File:\" + color.END, file)\n",
    "print(\n",
    "    color.BOLD + \"File Keys are\",\n",
    "    len(tree.keys()),\n",
    "    \"elements and are:\" + color.END,\n",
    "    file.keys(),\n",
    ")\n",
    "print(color.BOLD + \"File Classnames:\" + color.END, file.classnames())\n",
    "\n",
    "print(color.BOLD + \"Tree:\" + color.END, tree)\n",
    "print(\n",
    "    color.BOLD + \"Tree Keys are\",\n",
    "    len(tree.keys()),\n",
    "    \"elements and are:\" + color.END,\n",
    "    tree.keys(),\n",
    ")\n",
    "\n",
    "print(color.BOLD + \"Branches:\" + color.END, branches)\n",
    "print(\n",
    "    color.BOLD + \"PV are\",\n",
    "    len(branches[\"PV_x\"]),\n",
    "    \"elements and are:\" + color.END,\n",
    "    branches[\"PV_x\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(color.BOLD + \"Mean:               \" + color.END, np.mean(branches[\"nMuon\"]))\n",
    "print(color.BOLD + \"Standard Deviation: \" + color.END, np.std(branches[\"nMuon\"]))\n",
    "print(color.BOLD + \"Minimum:            \" + color.END, np.min(branches[\"nMuon\"]))\n",
    "print(color.BOLD + \"Maximum:            \" + color.END, np.max(branches[\"nMuon\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(branches[\"nMuon\"], bins=10, range=(0, 10))\n",
    "plt.xlabel(\"Number of muons in event\")\n",
    "plt.ylabel(\"Number of events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(branches.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract all data anyways and then use the ones we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not read:\n",
    "    for branch_name in branches.fields:\n",
    "        print(branch_name)\n",
    "        with open(f\"./datasets/ttbar/{branch_name}.txt\", \"w\") as f:\n",
    "            for value in branches[branch_name]:\n",
    "                f.write(f\"{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "For primary vertex identification in ttbar events, the key features to consider are\n",
    "\n",
    "1. Primary vertex coordinates: `PV_x`, `PV_y`, `PV_z`\n",
    "2. Track-related features from muons (since they're good for vertex identification):\n",
    "\n",
    "    - Impact parameter: `Muon_dxy`, `Muon_dxyErr`\n",
    "    - Longitudinal impact parameter: `Muon_dz`, `Muon_dzErr`\n",
    "    - Kinematics: `Muon_pt`, `Muon_eta`, `Muon_phi`\n",
    "\n",
    "3. Jet features (can help identify the hard scatter vertex): `Jet_pt`, `Jet_eta`, `Jet_phi`\n",
    "4. Number of primary vertices `PV_npvs` as auxiliary information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PV_x = branches[\"PV_x\"]\n",
    "PV_y = branches[\"PV_y\"]\n",
    "PV_z = branches[\"PV_z\"]\n",
    "PV_npvs = branches[\"PV_npvs\"]\n",
    "print(color.BOLD + \"PV_x are \" + f\"{len(PV_x)}:\" + color.END, PV_x)\n",
    "print(color.BOLD + \"PV_y are \" + f\"{len(PV_y)}:\" + color.END, PV_y)\n",
    "print(color.BOLD + \"PV_z are \" + f\"{len(PV_z)}:\" + color.END, PV_z)\n",
    "print(color.BOLD + \"PV_npvs are \" + f\"{len(PV_npvs)}:\" + color.END, PV_npvs)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "Muon_pt = branches[\"Muon_pt\"]\n",
    "Muon_eta = branches[\"Muon_eta\"]\n",
    "Muon_phi = branches[\"Muon_phi\"]\n",
    "Muon_dxy = branches[\"Muon_dxy\"]\n",
    "Muon_dxyErr = branches[\"Muon_dxyErr\"]\n",
    "Muon_dz = branches[\"Muon_dz\"]\n",
    "Muon_dzErr = branches[\"Muon_dzErr\"]\n",
    "print(color.BOLD + \"Muon_pt are \" + f\"{len(Muon_pt)}:\" + color.END, Muon_pt)\n",
    "print(color.BOLD + \"Muon_eta are \" + f\"{len(Muon_eta)}:\" + color.END, Muon_eta)\n",
    "print(color.BOLD + \"Muon_phi are \" + f\"{len(Muon_phi)}:\" + color.END, Muon_phi)\n",
    "print(color.BOLD + \"Muon_dxy are \" + f\"{len(Muon_dxy)}:\" + color.END, Muon_dxy)\n",
    "print(color.BOLD + \"Muon_dxyErr are \" + f\"{len(Muon_dxyErr)}:\" + color.END, Muon_dxyErr)\n",
    "print(color.BOLD + \"Muon_dz are \" + f\"{len(Muon_dz)}:\" + color.END, Muon_dz)\n",
    "print(color.BOLD + \"Muon_dzErr are \" + f\"{len(Muon_dzErr)}:\" + color.END, Muon_dzErr)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "Jet_pt = branches[\"Jet_pt\"]\n",
    "Jet_eta = branches[\"Jet_eta\"]\n",
    "Jet_phi = branches[\"Jet_phi\"]\n",
    "print(color.BOLD + \"Jet_pt are \" + f\"{len(Jet_pt)}:\" + color.END, Jet_pt)\n",
    "print(color.BOLD + \"Jet_eta are \" + f\"{len(Jet_eta)}:\" + color.END, Jet_eta)\n",
    "print(color.BOLD + \"Jet_phi are \" + f\"{len(Jet_phi)}:\" + color.END, Jet_phi)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "MET_pt = branches[\"MET_pt\"]\n",
    "MET_phi = branches[\"MET_phi\"]\n",
    "print(color.BOLD + \"MET_pt are \" + f\"{len(MET_pt)}:\" + color.END, MET_pt)\n",
    "print(color.BOLD + \"MET_phi are \" + f\"{len(MET_phi)}:\" + color.END, MET_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "PV = {\n",
    "    \"PV_x\": PV_x,\n",
    "    \"PV_y\": PV_y,\n",
    "    \"PV_z\": PV_z,\n",
    "    \"PV_npvs\": PV_npvs,\n",
    "}\n",
    "\n",
    "Muon = {\n",
    "    \"Muon_pt\": Muon_pt,\n",
    "    \"Muon_eta\": Muon_eta,\n",
    "    \"Muon_phi\": Muon_phi,\n",
    "    \"Muon_dxy\": Muon_dxy,\n",
    "    \"Muon_dxyErr\": Muon_dxyErr,\n",
    "    \"Muon_dz\": Muon_dz,\n",
    "    \"Muon_dzErr\": Muon_dzErr,\n",
    "}\n",
    "\n",
    "Jet = {\n",
    "    \"Jet_pt\": Jet_pt,\n",
    "    \"Jet_eta\": Jet_eta,\n",
    "    \"Jet_phi\": Jet_phi,\n",
    "}\n",
    "\n",
    "\n",
    "PVdf = pd.DataFrame(PV)\n",
    "Muondf = pd.DataFrame(Muon)\n",
    "Jetdf = pd.DataFrame(Jet)\n",
    "print(PVdf.head())\n",
    "print(Muondf.head())\n",
    "print(Jetdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def prepare_data(data, max_muons=5, max_jets=8):\n",
    "    \"\"\"\n",
    "    Prepare features for the model.\n",
    "    Args:\n",
    "        data: Dictionary containing the dataset\n",
    "        max_muons: Maximum number of muons to consider\n",
    "        max_jets: Maximum number of jets to consider\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "\n",
    "    # Add PV information\n",
    "    features_list.extend([data[\"PV_x\"], data[\"PV_y\"], data[\"PV_z\"], data[\"PV_npvs\"]])\n",
    "\n",
    "    # Add Muon information (for up to max_muons)\n",
    "    for i in range(max_muons):\n",
    "        if i < len(data[\"Muon_pt\"]):\n",
    "            features_list.extend(\n",
    "                [\n",
    "                    data[\"Muon_pt\"][i],\n",
    "                    data[\"Muon_eta\"][i],\n",
    "                    data[\"Muon_phi\"][i],\n",
    "                    data[\"Muon_dxy\"][i],\n",
    "                    data[\"Muon_dxyErr\"][i],\n",
    "                    data[\"Muon_dz\"][i],\n",
    "                    data[\"Muon_dzErr\"][i],\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # Padding for missing muons\n",
    "            features_list.extend([0] * 7)\n",
    "\n",
    "    # Add Jet information (for up to max_jets)\n",
    "    for i in range(max_jets):\n",
    "        if i < len(data[\"Jet_pt\"]):\n",
    "            features_list.extend(\n",
    "                [data[\"Jet_pt\"][i], data[\"Jet_eta\"][i], data[\"Jet_phi\"][i]]\n",
    "            )\n",
    "        else:\n",
    "            # Padding for missing jets\n",
    "            features_list.extend([0] * 3)\n",
    "\n",
    "    return np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(branches)\n",
    "\n",
    "# if read:\n",
    "#     columns = [\n",
    "#         \"PV_x\",\n",
    "#         \"PV_y\",\n",
    "#         \"PV_z\",\n",
    "#         \"Muon_dxy\",\n",
    "#         \"Muon_dxyErr\",\n",
    "#         \"Muon_dz\",\n",
    "#         \"Muon_dzErr\",\n",
    "#         \"Muon_pt\",\n",
    "#         \"Muon_eta\",\n",
    "#         \"Muon_phi\",\n",
    "#         \"Jet_pt\",\n",
    "#         \"Jet_eta\",\n",
    "#         \"Jet_phi\",\n",
    "#         \"PV_npvs\",\n",
    "#     ]\n",
    "\n",
    "#     dataframes = [\n",
    "#         pd.read_csv(\n",
    "#             f\"./datasets/ttbar/{col}.txt\",\n",
    "#             header=None,\n",
    "#             names=[col],\n",
    "#             delim_whitespace=True,\n",
    "#         )\n",
    "#         for col in columns\n",
    "#     ]\n",
    "#     data = pd.concat(dataframes, axis=1)\n",
    "\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def engineer_vertex_features(df):\n",
    "    \"\"\"\n",
    "    Engineer additional features for vertex identification.\n",
    "    \"\"\"\n",
    "    # Track quality metrics\n",
    "    df[\"track_quality\"] = df[\"Muon_pt\"] / df[\"Muon_ptErr\"]\n",
    "\n",
    "    # Impact parameter significance\n",
    "    df[\"dxy_significance\"] = df[\"Muon_dxy\"] / df[\"Muon_dxyErr\"]\n",
    "    df[\"dz_significance\"] = df[\"Muon_dz\"] / df[\"Muon_dzErr\"]\n",
    "\n",
    "    # Angular separation between tracks\n",
    "    df[\"delta_phi\"] = abs(df[\"Muon_phi\"] - df[\"Jet_phi\"])\n",
    "    df[\"delta_eta\"] = abs(df[\"Muon_eta\"] - df[\"Jet_eta\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Pile-up Mitigation\n",
    "def pileup_weighting(df):\n",
    "    \"\"\"\n",
    "    Apply pile-up dependent weights\n",
    "    \"\"\"\n",
    "    # Example weighting based on number of vertices\n",
    "    weights = 1.0 / (1.0 + 0.1 * df[\"PV_npvs\"])\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Quality Cuts\n",
    "def apply_quality_cuts(df):\n",
    "    \"\"\"\n",
    "    Apply basic quality cuts for vertex identification\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        (abs(df[\"Muon_dxy\"]) < 0.2)  # Impact parameter cut\n",
    "        & (df[\"Muon_pt\"] > 20)  # Minimum pt cut\n",
    "        & (abs(df[\"Muon_eta\"]) < 2.4)  # Eta acceptance\n",
    "        & (df[\"dxy_significance\"] < 5)  # Impact parameter significance\n",
    "    )\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "# Track Clustering\n",
    "def cluster_tracks(df, max_dz=0.2):\n",
    "    \"\"\"\n",
    "    Simple track clustering by z-position\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import DBSCAN\n",
    "\n",
    "    # Prepare features for clustering\n",
    "    X = np.vstack([df[\"PV_z\"], df[\"Muon_dz\"]]).T\n",
    "\n",
    "    # Perform clustering\n",
    "    clustering = DBSCAN(eps=max_dz, min_samples=2).fit(X)\n",
    "\n",
    "    return clustering.labels_\n",
    "\n",
    "\n",
    "# Vertex Scoring\n",
    "def score_vertex_candidates(df):\n",
    "    \"\"\"\n",
    "    Score vertex candidates based on various criteria\n",
    "    \"\"\"\n",
    "    scores = (\n",
    "        df[\"track_quality\"] * 0.3\n",
    "        + (1.0 / df[\"dxy_significance\"]) * 0.3\n",
    "        + (df[\"Muon_pt\"] / df[\"Muon_pt\"].max()) * 0.4\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture:**\n",
    "\n",
    "-   Deep neural network with 5 layers (256→128→64→32→1)\n",
    "-   Includes dropout for regularization\n",
    "-   Batch normalization for better training stability\n",
    "-   ReLU activation functions\n",
    "-   Sigmoid output for binary classification\n",
    "\n",
    "**Training Pipeline:**\n",
    "\n",
    "-   Custom Dataset class for efficient data handling\n",
    "-   Training loop with validation\n",
    "-   Adam optimizer and Binary Cross Entropy loss\n",
    "-   Built-in progress monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features=10,  # 7 track + 3 jet features\n",
    "        edge_features=2,  # η-φ distance features\n",
    "        global_features=3,  # PV_npvs + MET features\n",
    "        hidden_dim=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Edge convolution layers\n",
    "        self.edge_conv1 = geom_nn.EdgeConv(\n",
    "            nn=nn.Sequential(\n",
    "                nn.Linear(2 * node_features + edge_features, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            ),\n",
    "            aggr=\"max\",\n",
    "        )\n",
    "\n",
    "        self.edge_conv2 = geom_nn.EdgeConv(\n",
    "            nn=nn.Sequential(\n",
    "                nn.Linear(2 * hidden_dim + edge_features, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            ),\n",
    "            aggr=\"max\",\n",
    "        )\n",
    "\n",
    "        self.edge_conv3 = geom_nn.EdgeConv(\n",
    "            nn=nn.Sequential(\n",
    "                nn.Linear(2 * hidden_dim + edge_features, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            ),\n",
    "            aggr=\"max\",\n",
    "        )\n",
    "\n",
    "        # Combine with global features\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + global_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Unpack the graph data\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        batch = data.batch if hasattr(data, \"batch\") else None\n",
    "        global_features = data.global_features\n",
    "\n",
    "        # Apply edge convolutions\n",
    "        x = self.edge_conv1(x, edge_index, edge_attr)\n",
    "        x = self.edge_conv2(x, edge_index, edge_attr)\n",
    "        x = self.edge_conv3(x, edge_index, edge_attr)\n",
    "\n",
    "        # Global pooling\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        x = scatter_mean(x, batch, dim=0)\n",
    "\n",
    "        # Combine with global features\n",
    "        x = torch.cat([x, global_features], dim=1)\n",
    "        x = self.global_mlp(x)\n",
    "\n",
    "        # Final classification\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# class VertexIdentificationGNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=128, output_dim=1):\n",
    "#         super(VertexIdentificationGNN, self).__init__()\n",
    "#         self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "#         self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "#         self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv3(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "\n",
    "#         # Global mean pooling\n",
    "#         x = global_mean_pool(x, batch)\n",
    "\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch)\n",
    "        loss = loss_fn(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "#     criterion = nn.BCELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         for batch_data, batch_labels in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(batch_data)\n",
    "#             loss = criterion(outputs, batch_labels.unsqueeze(1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch_data, batch_labels in val_loader:\n",
    "#                 outputs = model(batch_data)\n",
    "#                 loss = criterion(outputs, batch_labels.unsqueeze(1))\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "#             print(\n",
    "#                 f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "#                 f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "#                 f\"Val Loss: {val_loss/len(val_loader):.4f}\"\n",
    "#             )\n",
    "\n",
    "\n",
    "# # Calculate input dimension\n",
    "# input_dim = 4 + (7 * 5) + (3 * 8)  # PV features + Muon features + Jet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph_data(tracks, jets, global_features):\n",
    "    \"\"\"\n",
    "    Prepare graph data structure from physics objects\n",
    "    \"\"\"\n",
    "    # Node features: [pT, η, φ, dxy, dxyErr, dz, dzErr] + [Jet_pT, Jet_eta, Jet_phi]\n",
    "    node_features = torch.cat([tracks, jets], dim=1)\n",
    "\n",
    "    # Calculate edges (fully connected graph)\n",
    "    num_nodes = node_features.size(0)\n",
    "    edge_index = torch.combinations(torch.arange(num_nodes), r=2).t()\n",
    "\n",
    "    # Calculate edge features (η-φ distances)\n",
    "    node_eta_phi = node_features[:, [1, 2]]  # η, φ columns\n",
    "    edge_attr = calc_delta_r(node_eta_phi[edge_index[0]], node_eta_phi[edge_index[1]])\n",
    "\n",
    "    return Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        global_features=global_features,\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_delta_r(point1, point2):\n",
    "    \"\"\"Calculate η-φ distance between points\"\"\"\n",
    "    delta_eta = point1[:, 0] - point2[:, 0]\n",
    "    delta_phi = torch.abs(point1[:, 1] - point2[:, 1])\n",
    "    delta_phi = torch.min(delta_phi, 2 * torch.pi - delta_phi)\n",
    "    return torch.stack([delta_eta, delta_phi], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VertexIdentificationModel(input_dim)\n",
    "\n",
    "# Prepare your data\n",
    "X_train = ...  # Your training data\n",
    "y_train = ...  # Your training labels\n",
    "X_val = ...  # Your validation data\n",
    "y_val = ...  # Your validation labels\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VertexDataset(X_train, y_train)\n",
    "val_dataset = VertexDataset(X_val, y_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DynamicGNN(nn.Module):\n",
    "#     def __init__(self, num_features):\n",
    "#         super().__init__()\n",
    "#         self.edge_conv1 = EdgeConv(\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(2 * num_features, 128),\n",
    "#                 nn.BatchNorm1d(128),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(128, 64),\n",
    "#             )\n",
    "#         )\n",
    "#         self.edge_conv2 = EdgeConv(\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(2 * 64, 256),\n",
    "#                 nn.BatchNorm1d(256),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(256, 128),\n",
    "#             )\n",
    "#         )\n",
    "#         self.edge_conv3 = EdgeConv(\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(2 * 128, 512),\n",
    "#                 nn.BatchNorm1d(512),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(512, 256),\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         self.node_update = nn.Sequential(\n",
    "#             nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Linear(128, 64)\n",
    "#         )\n",
    "\n",
    "#         self.global_pool = global_max_pool\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 1), nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         # x: (N, num_features), edge_index: (2, E)\n",
    "\n",
    "#         x = self.edge_conv1(x, edge_index)\n",
    "#         x = self.edge_conv2(x, edge_index)\n",
    "#         x = self.edge_conv3(x, edge_index)\n",
    "\n",
    "#         x = self.node_update(x)\n",
    "\n",
    "#         graph_feature = self.global_pool(\n",
    "#             x, torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "#         )\n",
    "\n",
    "#         out = self.classifier(graph_feature)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# model = DynamicGNN(num_features=13)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "\n",
    "#     # Prepare input data\n",
    "#     track_features = ...  # (N, 13)\n",
    "#     edge_index = ...  # (2, E)\n",
    "\n",
    "#     output = model(track_features, edge_index)\n",
    "#     loss = criterion(output, target)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Evaluation\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         eval_output = model(eval_track_features, eval_edge_index)\n",
    "#         eval_loss = criterion(eval_output, eval_target)\n",
    "\n",
    "#         # Calculate evaluation metrics\n",
    "#         # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Var 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import EdgeConv, global_max_pool, global_mean_pool\n",
    "# from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "\n",
    "# class DynamicEdgeConv(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(2 * in_channels, 2 * out_channels),\n",
    "#             nn.BatchNorm1d(2 * out_channels),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(2 * out_channels, out_channels),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         row, col = edge_index\n",
    "#         edge_attr = torch.cat([x[row], x[col]], dim=1)\n",
    "#         return self.mlp(edge_attr)\n",
    "\n",
    "\n",
    "# class DynamicGNN(nn.Module):\n",
    "#     def __init__(self, num_features, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.edge_conv1 = DynamicEdgeConv(num_features, 64)\n",
    "#         self.edge_conv2 = DynamicEdgeConv(64, 128)\n",
    "#         self.edge_conv3 = DynamicEdgeConv(128, 256)\n",
    "\n",
    "#         self.node_update1 = nn.Sequential(\n",
    "#             nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Linear(128, 64)\n",
    "#         )\n",
    "#         self.node_update2 = nn.Sequential(\n",
    "#             nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Linear(32, 16)\n",
    "#         )\n",
    "\n",
    "#         self.global_pool = global_max_pool\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, num_classes), nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         # x: (N, num_features), edge_index: (2, E)\n",
    "\n",
    "#         x = self.edge_conv1(x, edge_index)\n",
    "#         x = self.edge_conv2(x, edge_index)\n",
    "#         x = self.edge_conv3(x, edge_index)\n",
    "\n",
    "#         x = self.node_update1(x)\n",
    "#         x = self.node_update2(x)\n",
    "\n",
    "#         graph_feature = self.global_pool(\n",
    "#             x, torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "#         )\n",
    "\n",
    "#         out = self.classifier(graph_feature)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# model = DynamicGNN(num_features=13, num_classes=2)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "\n",
    "#     # Prepare input data\n",
    "#     track_features = ...  # (N, 13)\n",
    "#     edge_index = ...  # (2, E)\n",
    "\n",
    "#     output = model(track_features, edge_index)\n",
    "#     loss = criterion(output, target)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Evaluation\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         eval_output = model(eval_track_features, eval_edge_index)\n",
    "#         eval_loss = criterion(eval_output, eval_target)\n",
    "\n",
    "#         # Calculate evaluation metrics\n",
    "#         # ...\n",
    "\n",
    "\n",
    "# # Dynamic Edge Construction\n",
    "# def get_dynamic_edges(track_features, k=20):\n",
    "#     \"\"\"\n",
    "#     Construct dynamic k-nearest neighbor edges based on track features.\n",
    "\n",
    "#     Args:\n",
    "#         track_features (torch.Tensor): (N, num_features)\n",
    "#         k (int): Number of nearest neighbors\n",
    "\n",
    "#     Returns:\n",
    "#         edge_index (torch.Tensor): (2, E)\n",
    "#     \"\"\"\n",
    "#     N = track_features.size(0)\n",
    "#     edge_index = []\n",
    "\n",
    "#     for i in range(N):\n",
    "#         dists = torch.sum((track_features - track_features[i]) ** 2, dim=1)\n",
    "#         _, indices = torch.topk(dists, k=k + 1)\n",
    "#         for j in indices[1:]:\n",
    "#             edge_index.append([i, j])\n",
    "#             edge_index.append([j, i])\n",
    "\n",
    "#     return torch.tensor(edge_index, dtype=torch.long).t()\n",
    "\n",
    "\n",
    "# # Vertex Classification\n",
    "# def classify_vertices(model, track_features, edge_index):\n",
    "#     \"\"\"\n",
    "#     Classify primary vertices using the Dynamic Graph CNN model.\n",
    "\n",
    "#     Args:\n",
    "#         model (DynamicGNN): Trained model\n",
    "#         track_features (torch.Tensor): (N, num_features)\n",
    "#         edge_index (torch.Tensor): (2, E)\n",
    "\n",
    "#     Returns:\n",
    "#         vertex_scores (torch.Tensor): (N,) Vertex classification scores\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         output = model(track_features, edge_index)\n",
    "#         vertex_scores = output.squeeze()\n",
    "#     return vertex_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Var 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GraphNeuralNetwork(nn.Module):\n",
    "#     def __init__(self, in_features, hidden_features, out_features):\n",
    "#         super(GraphNeuralNetwork, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_features, hidden_features)\n",
    "#         self.conv2 = GCNConv(hidden_features, out_features)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
